{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Machine Translation Project (English to French)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import pad_sequences\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Input, Dense, Embedding, GRU, LSTM, Bidirectional, Dropout, Activation, TimeDistributed, RepeatVector\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import sparse_categorical_crossentropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify access to the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 5592121452488144900\n",
      "xla_global_id: -1\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 2238133044\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 2611008541144377426\n",
      "physical_device_desc: \"device: 0, name: NVIDIA GeForce GTX 1650, pci bus id: 0000:01:00.0, compute capability: 7.5\"\n",
      "xla_global_id: 416903419\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    input_file = path\n",
    "    with open(input_file, \"r\") as f:\n",
    "        data = f.read()\n",
    "    return data.split('\\n')\n",
    "\n",
    "english_sentences = load_data('data/english')\n",
    "french_sentences = load_data('data/french')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['new jersey is sometimes quiet during autumn , and it is snowy in april .',\n",
       " 'the united states is usually chilly during july , and it is usually freezing in november .',\n",
       " 'california is usually quiet during march , and it is usually hot in june .',\n",
       " 'the united states is sometimes mild during june , and it is cold in september .',\n",
       " 'your least liked fruit is the grape , but my least liked is the apple .']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_sentences[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Structure of the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1823250 English words.\n",
      "227 unique English words.\n",
      "10 Most common words in the English dataset:\n",
      "\"is\" \",\" \".\" \"in\" \"it\" \"during\" \"the\" \"but\" \"and\" \"sometimes\"\n",
      "\n",
      "1961295 French words.\n",
      "355 unique French words.\n",
      "10 Most common words in the French dataset:\n",
      "\"est\" \".\" \",\" \"en\" \"il\" \"les\" \"mais\" \"et\" \"la\" \"parfois\"\n"
     ]
    }
   ],
   "source": [
    "english_words_counter = collections.Counter([word for sentence in english_sentences for word in sentence.split()])\n",
    "french_words_counter = collections.Counter([word for sentence in french_sentences for word in sentence.split()])\n",
    "\n",
    "print('{} English words.'.format(len([word for sentence in english_sentences for word in sentence.split()])))\n",
    "print('{} unique English words.'.format(len(english_words_counter)))\n",
    "print('10 Most common words in the English dataset:')\n",
    "print('\"' + '\" \"'.join(list(zip(*english_words_counter.most_common(10)))[0]) + '\"')\n",
    "\n",
    "print()\n",
    "print('{} French words.'.format(len([word for sentence in french_sentences for word in sentence.split()])))\n",
    "print('{} unique French words.'.format(len(french_words_counter)))\n",
    "print('10 Most common words in the French dataset:')\n",
    "print('\"' + '\" \"'.join(list(zip(*french_words_counter.most_common(10)))[0]) + '\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess\n",
    "1. Tokenize the words into ids\n",
    "2. Add padding to make all the sequences the same length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(x):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(x)\n",
    "    return tokenizer.texts_to_sequences(x), tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad(x, length=None):\n",
    "    if length is None:\n",
    "        length = max([len(sentence) for sentence in x])\n",
    "    return pad_sequences(x, maxlen=length, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Preprocessed\n",
      "Max English sentence length: 15\n",
      "Max French sentence length: 21\n",
      "English vocabulary size: 199\n",
      "French vocabulary size: 344\n"
     ]
    }
   ],
   "source": [
    "def preprocess(x,y):\n",
    "    preprocess_x, x_tk = tokenize(x)\n",
    "    preprocess_y, y_tk = tokenize(y)\n",
    "\n",
    "    \n",
    "    preprocess_x = pad(preprocess_x)\n",
    "    preprocess_y = pad(preprocess_y)\n",
    "    \n",
    "    preprocess_y = preprocess_y.reshape(*preprocess_y.shape, 1)\n",
    "    \n",
    "    return preprocess_x, preprocess_y, x_tk, y_tk\n",
    "\n",
    "preproc_english_sentences, preproc_french_sentences, english_tokenizer, french_tokenizer = preprocess(english_sentences, french_sentences)\n",
    "\n",
    "max_english_sequence_length = preproc_english_sentences.shape[1]\n",
    "max_french_sequence_length = preproc_french_sentences.shape[1]\n",
    "english_vocab_size = len(english_tokenizer.word_index)\n",
    "french_vocab_size = len(french_tokenizer.word_index)\n",
    "\n",
    "print('Data Preprocessed')\n",
    "print(\"Max English sentence length:\", max_english_sequence_length)\n",
    "print(\"Max French sentence length:\", max_french_sequence_length)\n",
    "print(\"English vocabulary size:\", english_vocab_size)\n",
    "print(\"French vocabulary size:\", french_vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models\n",
    "- Model 1 is a simple RNN\n",
    "- Model 2 is a Bidirectional RNN\n",
    "- Model 3 is an Embedding RNN\n",
    "\n",
    "### Ids Back to Text\n",
    "The neural network will be translating the input to words ids, which isn't the final form we want.  We want the French translation.  The function `logits_to_text` will bridge the gab between the logits from the neural network to the French translation.  You'll be using this function to better understand the output of the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logits_to_text(logits, tokenizer):\n",
    "    index_to_words = {id: word for word, id in tokenizer.word_index.items()}\n",
    "    index_to_words[0] = '<PAD>'\n",
    "    \n",
    "    return ' '.join([index_to_words[prediction] for prediction in np.argmax(logits, 1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1: RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      " 69/108 [==================>...........] - ETA: 4s - loss: 2.1968 - accuracy: 0.5076"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 30\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m#Train the neural network\u001b[39;00m\n\u001b[0;32m     24\u001b[0m simple_rnn_model \u001b[38;5;241m=\u001b[39m simple_model(\n\u001b[0;32m     25\u001b[0m     tmp_x\u001b[38;5;241m.\u001b[39mshape,\n\u001b[0;32m     26\u001b[0m     max_french_sequence_length,\n\u001b[0;32m     27\u001b[0m     english_vocab_size,\n\u001b[0;32m     28\u001b[0m     french_vocab_size)\n\u001b[1;32m---> 30\u001b[0m \u001b[43msimple_rnn_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtmp_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreproc_french_sentences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\adity\\anaconda3\\envs\\nullclass\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\adity\\anaconda3\\envs\\nullclass\\lib\\site-packages\\keras\\engine\\training.py:1564\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1556\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1557\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1558\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1561\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   1562\u001b[0m ):\n\u001b[0;32m   1563\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1564\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1565\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1566\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mc:\\Users\\adity\\anaconda3\\envs\\nullclass\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\adity\\anaconda3\\envs\\nullclass\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\adity\\anaconda3\\envs\\nullclass\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    945\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    946\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 947\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateless_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    948\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    949\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    950\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    951\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\Users\\adity\\anaconda3\\envs\\nullclass\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2496\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2493\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m   2494\u001b[0m   (graph_function,\n\u001b[0;32m   2495\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2496\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2497\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\adity\\anaconda3\\envs\\nullclass\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1862\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1858\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1859\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1860\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1861\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1862\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1863\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1864\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1865\u001b[0m     args,\n\u001b[0;32m   1866\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1867\u001b[0m     executing_eagerly)\n\u001b[0;32m   1868\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\adity\\anaconda3\\envs\\nullclass\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    498\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 499\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    505\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    506\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    507\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m    508\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    511\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[0;32m    512\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32mc:\\Users\\adity\\anaconda3\\envs\\nullclass\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def simple_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n",
    "    \n",
    "    #Hyperparameters\n",
    "    learning_rate = 0.005\n",
    "    \n",
    "    # Build the layers\n",
    "    model = Sequential()\n",
    "    model.add(GRU(256, input_shape=input_shape[1:], return_sequences=True))\n",
    "    model.add(TimeDistributed(Dense(1024, activation='relu')))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(TimeDistributed(Dense(french_vocab_size, activation='softmax')))\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(loss = sparse_categorical_crossentropy,\n",
    "                  optimizer = Adam(learning_rate),\n",
    "                  metrics = ['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "tmp_x = pad(preproc_english_sentences, max_french_sequence_length)\n",
    "tmp_x = tmp_x.reshape((-1, preproc_french_sentences.shape[-2], 1))\n",
    "\n",
    "#Train the neural network\n",
    "simple_rnn_model = simple_model(\n",
    "    tmp_x.shape,\n",
    "    max_french_sequence_length,\n",
    "    english_vocab_size,\n",
    "    french_vocab_size)\n",
    "\n",
    "simple_rnn_model.fit(tmp_x, preproc_french_sentences, batch_size=1024, epochs=10, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediciton:\n",
      "1/1 [==============================] - 0s 448ms/step\n",
      "new jersey est parfois calme en l' et il est il est en en <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "\n",
      "Correct Translation:\n",
      "[\"new jersey est parfois calme pendant l' automne , et il est neigeux en avril .\"]\n",
      "\n",
      "Original text:\n",
      "['new jersey is sometimes quiet during autumn , and it is snowy in april .']\n"
     ]
    }
   ],
   "source": [
    "# Print prediction(s)\n",
    "print(\"Prediciton:\")\n",
    "print(logits_to_text(simple_rnn_model.predict(tmp_x[:1])[0], french_tokenizer))\n",
    "\n",
    "print(\"\\nCorrect Translation:\")\n",
    "print(french_sentences[:1])\n",
    "\n",
    "print('\\nOriginal text:')\n",
    "print(english_sentences[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2: Bidirectional RNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bidirectional (Bidirectiona  (None, 21, 256)          100608    \n",
      " l)                                                              \n",
      "                                                                 \n",
      " time_distributed_2 (TimeDis  (None, 21, 1024)         263168    \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 21, 1024)          0         \n",
      "                                                                 \n",
      " time_distributed_3 (TimeDis  (None, 21, 344)          352600    \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 716,376\n",
      "Trainable params: 716,376\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/10\n",
      "108/108 [==============================] - 10s 64ms/step - loss: 1.7555 - accuracy: 0.5752 - val_loss: nan - val_accuracy: 0.6471\n",
      "Epoch 2/10\n",
      "108/108 [==============================] - 7s 67ms/step - loss: 1.1732 - accuracy: 0.6546 - val_loss: nan - val_accuracy: 0.6803\n",
      "Epoch 3/10\n",
      "108/108 [==============================] - 8s 77ms/step - loss: 1.0462 - accuracy: 0.6760 - val_loss: nan - val_accuracy: 0.6886\n",
      "Epoch 4/10\n",
      "108/108 [==============================] - 8s 74ms/step - loss: 0.9696 - accuracy: 0.6871 - val_loss: nan - val_accuracy: 0.7018\n",
      "Epoch 5/10\n",
      "108/108 [==============================] - 8s 73ms/step - loss: 0.9159 - accuracy: 0.6962 - val_loss: nan - val_accuracy: 0.7112\n",
      "Epoch 6/10\n",
      "108/108 [==============================] - 8s 72ms/step - loss: 0.8750 - accuracy: 0.7038 - val_loss: nan - val_accuracy: 0.7172\n",
      "Epoch 7/10\n",
      "108/108 [==============================] - 8s 71ms/step - loss: 0.8414 - accuracy: 0.7113 - val_loss: nan - val_accuracy: 0.7287\n",
      "Epoch 8/10\n",
      "108/108 [==============================] - 8s 74ms/step - loss: 0.8161 - accuracy: 0.7163 - val_loss: nan - val_accuracy: 0.7368\n",
      "Epoch 9/10\n",
      "108/108 [==============================] - 9s 80ms/step - loss: 0.7915 - accuracy: 0.7216 - val_loss: nan - val_accuracy: 0.7339\n",
      "Epoch 10/10\n",
      "108/108 [==============================] - 10s 92ms/step - loss: 0.7849 - accuracy: 0.7222 - val_loss: nan - val_accuracy: 0.7424\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1220e9f6c10>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def bd_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n",
    "    \n",
    "    #Hyperparameters\n",
    "    learning_rate = 0.005\n",
    "    \n",
    "    # Build the layers\n",
    "    model = Sequential()\n",
    "    model.add(Bidirectional(GRU(128, return_sequences=True), input_shape=input_shape[1:]))\n",
    "    model.add(TimeDistributed(Dense(1024, activation='relu')))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(TimeDistributed(Dense(french_vocab_size, activation='softmax')))\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(loss = sparse_categorical_crossentropy,\n",
    "                  optimizer = Adam(learning_rate),\n",
    "                  metrics = ['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "tmp_x = pad(preproc_english_sentences, max_french_sequence_length)\n",
    "tmp_x = tmp_x.reshape((-1, preproc_french_sentences.shape[-2], 1))\n",
    "\n",
    "# Train the neural network\n",
    "bd_rnn_model = bd_model(\n",
    "    tmp_x.shape,\n",
    "    max_french_sequence_length,\n",
    "    english_vocab_size,\n",
    "    french_vocab_size)\n",
    "\n",
    "print(bd_rnn_model.summary())\n",
    "\n",
    "bd_rnn_model.fit(tmp_x, preproc_french_sentences, batch_size=1024, epochs=10, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediciton:\n",
      "1/1 [==============================] - 1s 767ms/step\n",
      "new jersey est parfois chaud en l' et il et il est en en <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "\n",
      "Correct Translation:\n",
      "[\"new jersey est parfois calme pendant l' automne , et il est neigeux en avril .\"]\n",
      "\n",
      "Original text:\n",
      "['new jersey is sometimes quiet during autumn , and it is snowy in april .']\n"
     ]
    }
   ],
   "source": [
    "# Print prediction(s)\n",
    "print(\"Prediciton:\")\n",
    "print(logits_to_text(bd_rnn_model.predict(tmp_x[:1])[0], french_tokenizer))\n",
    "\n",
    "print(\"\\nCorrect Translation:\")\n",
    "print(french_sentences[:1])\n",
    "\n",
    "print('\\nOriginal text:')\n",
    "print(english_sentences[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3: Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_3 (Embedding)     (None, 21, 256)           50944     \n",
      "                                                                 \n",
      " bidirectional_3 (Bidirectio  (None, 21, 512)          789504    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " time_distributed_8 (TimeDis  (None, 21, 1024)         525312    \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 21, 1024)          0         \n",
      "                                                                 \n",
      " time_distributed_9 (TimeDis  (None, 21, 344)          352600    \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,718,360\n",
      "Trainable params: 1,718,360\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/10\n",
      "108/108 [==============================] - 28s 231ms/step - loss: 1.2279 - accuracy: 0.7006 - val_loss: nan - val_accuracy: 0.8789\n",
      "Epoch 2/10\n",
      "108/108 [==============================] - 26s 245ms/step - loss: 0.2910 - accuracy: 0.9063 - val_loss: nan - val_accuracy: 0.9387\n",
      "Epoch 3/10\n",
      "108/108 [==============================] - 26s 244ms/step - loss: 0.1692 - accuracy: 0.9463 - val_loss: nan - val_accuracy: 0.9621\n",
      "Epoch 4/10\n",
      "108/108 [==============================] - 26s 243ms/step - loss: 0.1197 - accuracy: 0.9625 - val_loss: nan - val_accuracy: 0.9700\n",
      "Epoch 5/10\n",
      "108/108 [==============================] - 26s 243ms/step - loss: 0.0958 - accuracy: 0.9702 - val_loss: nan - val_accuracy: 0.9760\n",
      "Epoch 6/10\n",
      "108/108 [==============================] - 26s 243ms/step - loss: 0.0823 - accuracy: 0.9746 - val_loss: nan - val_accuracy: 0.9765\n",
      "Epoch 7/10\n",
      "108/108 [==============================] - 26s 244ms/step - loss: 0.0703 - accuracy: 0.9782 - val_loss: nan - val_accuracy: 0.9806\n",
      "Epoch 8/10\n",
      "108/108 [==============================] - 24s 226ms/step - loss: 0.0605 - accuracy: 0.9812 - val_loss: nan - val_accuracy: 0.9820\n",
      "Epoch 9/10\n",
      "108/108 [==============================] - 26s 239ms/step - loss: 0.0559 - accuracy: 0.9827 - val_loss: nan - val_accuracy: 0.9824\n",
      "Epoch 10/10\n",
      "108/108 [==============================] - 24s 226ms/step - loss: 0.0530 - accuracy: 0.9836 - val_loss: nan - val_accuracy: 0.9825\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x268a8948f40>"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def bidirectional_embed_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n",
    "    \n",
    "    # Hyperparameters\n",
    "    learning_rate = 0.005\n",
    "    \n",
    "    # Build the layers\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(english_vocab_size, 256, input_length=input_shape[1], input_shape=input_shape[1:]))\n",
    "    model.add(Bidirectional(GRU(256, return_sequences=True)))\n",
    "    model.add(TimeDistributed(Dense(1024, activation='relu')))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(TimeDistributed(Dense(french_vocab_size, activation='softmax')))\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(loss = sparse_categorical_crossentropy,\n",
    "                  optimizer = Adam(learning_rate),\n",
    "                  metrics = ['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "tmp_x = pad(preproc_english_sentences, max_french_sequence_length)\n",
    "tmp_x = tmp_x.reshape((-1, preproc_french_sentences.shape[-2]))\n",
    "\n",
    "# Build the model\n",
    "embed_rnn_model = bidirectional_embed_model(\n",
    "    tmp_x.shape,\n",
    "    max_french_sequence_length,\n",
    "    english_vocab_size,\n",
    "    french_vocab_size)\n",
    "\n",
    "print(embed_rnn_model.summary())\n",
    "\n",
    "embed_rnn_model.fit(tmp_x, preproc_french_sentences, batch_size=1024, epochs=10, validation_split=0.2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediciton:\n",
      "1/1 [==============================] - 1s 584ms/step\n",
      "new jersey est parfois calme pendant l'automne automne et est neigeux en avril <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "\n",
      "Correct Translation:\n",
      "[\"new jersey est parfois calme pendant l' automne , et il est neigeux en avril .\"]\n",
      "\n",
      "Original text:\n",
      "['new jersey is sometimes quiet during autumn , and it is snowy in april .']\n"
     ]
    }
   ],
   "source": [
    "print(\"Prediciton:\")\n",
    "print(logits_to_text(embed_rnn_model.predict(tmp_x[:1])[0], french_tokenizer))\n",
    "\n",
    "print(\"\\nCorrect Translation:\")\n",
    "print(french_sentences[:1])\n",
    "\n",
    "print('\\nOriginal text:')\n",
    "print(english_sentences[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as gru_cell_26_layer_call_fn, gru_cell_26_layer_call_and_return_conditional_losses, gru_cell_27_layer_call_fn, gru_cell_27_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: english_to_french_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: english_to_french_model\\assets\n"
     ]
    }
   ],
   "source": [
    "embed_rnn_model.save('english_to_french_model')\n",
    "# Serialize English Tokenizer to JSON\n",
    "with open('english_tokenizer.json', 'w', encoding='utf8') as f:\n",
    "    f.write(json.dumps(english_tokenizer.to_json(), ensure_ascii=False))\n",
    "    \n",
    "# Serialize French Tokenizer to JSON\n",
    "with open('french_tokenizer.json', 'w', encoding='utf8') as f:\n",
    "    f.write(json.dumps(french_tokenizer.to_json(), ensure_ascii=False))\n",
    "    \n",
    "# Save max lengths\n",
    "max_french_sequence_length_json = max_french_sequence_length\n",
    "with open('sequence_length.json', 'w', encoding='utf8') as f:\n",
    "    f.write(json.dumps(max_french_sequence_length_json, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Internship Tasks:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Write a Python function to implement a basic tokenization algorithm for a given language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We tokenize the english text corpus above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start as the Python Environment 'nullclass (Python 3.11.9)' is no longer available. Consider selecting another kernel or refreshing the list of Python Environments."
     ]
    }
   ],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = []\n",
    "\n",
    "for sentence in english_sentences:\n",
    "    sentence = sentence.translate(str.maketrans('', '', string.punctuation))\n",
    "    words_in_sentence = sentence.lower().split()\n",
    "\n",
    "    for word in words_in_sentence:\n",
    "        tokens.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab, index = {}, 1  # start indexing from 1\n",
    "vocab['<pad>'] = 0  # add a padding token\n",
    "for token in tokens:\n",
    "  if token not in vocab:\n",
    "    vocab[token] = index\n",
    "    index += 1\n",
    "vocab_size = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '<pad>',\n",
       " 1: 'new',\n",
       " 2: 'jersey',\n",
       " 3: 'is',\n",
       " 4: 'sometimes',\n",
       " 5: 'quiet',\n",
       " 6: 'during',\n",
       " 7: 'autumn',\n",
       " 8: 'and',\n",
       " 9: 'it',\n",
       " 10: 'snowy',\n",
       " 11: 'in',\n",
       " 12: 'april',\n",
       " 13: 'the',\n",
       " 14: 'united',\n",
       " 15: 'states',\n",
       " 16: 'usually',\n",
       " 17: 'chilly',\n",
       " 18: 'july',\n",
       " 19: 'freezing',\n",
       " 20: 'november',\n",
       " 21: 'california',\n",
       " 22: 'march',\n",
       " 23: 'hot',\n",
       " 24: 'june',\n",
       " 25: 'mild',\n",
       " 26: 'cold',\n",
       " 27: 'september',\n",
       " 28: 'your',\n",
       " 29: 'least',\n",
       " 30: 'liked',\n",
       " 31: 'fruit',\n",
       " 32: 'grape',\n",
       " 33: 'but',\n",
       " 34: 'my',\n",
       " 35: 'apple',\n",
       " 36: 'his',\n",
       " 37: 'favorite',\n",
       " 38: 'orange',\n",
       " 39: 'paris',\n",
       " 40: 'relaxing',\n",
       " 41: 'december',\n",
       " 42: 'busy',\n",
       " 43: 'spring',\n",
       " 44: 'never',\n",
       " 45: 'our',\n",
       " 46: 'lemon',\n",
       " 47: 'january',\n",
       " 48: 'warm',\n",
       " 49: 'lime',\n",
       " 50: 'her',\n",
       " 51: 'banana',\n",
       " 52: 'he',\n",
       " 53: 'saw',\n",
       " 54: 'a',\n",
       " 55: 'old',\n",
       " 56: 'yellow',\n",
       " 57: 'truck',\n",
       " 58: 'india',\n",
       " 59: 'rainy',\n",
       " 60: 'that',\n",
       " 61: 'cat',\n",
       " 62: 'was',\n",
       " 63: 'most',\n",
       " 64: 'loved',\n",
       " 65: 'animal',\n",
       " 66: 'dislikes',\n",
       " 67: 'grapefruit',\n",
       " 68: 'limes',\n",
       " 69: 'lemons',\n",
       " 70: 'february',\n",
       " 71: 'china',\n",
       " 72: 'pleasant',\n",
       " 73: 'october',\n",
       " 74: 'wonderful',\n",
       " 75: 'nice',\n",
       " 76: 'summer',\n",
       " 77: 'france',\n",
       " 78: 'may',\n",
       " 79: 'grapes',\n",
       " 80: 'mangoes',\n",
       " 81: 'their',\n",
       " 82: 'mango',\n",
       " 83: 'pear',\n",
       " 84: 'august',\n",
       " 85: 'beautiful',\n",
       " 86: 'apples',\n",
       " 87: 'peaches',\n",
       " 88: 'feared',\n",
       " 89: 'shark',\n",
       " 90: 'wet',\n",
       " 91: 'dry',\n",
       " 92: 'we',\n",
       " 93: 'like',\n",
       " 94: 'oranges',\n",
       " 95: 'they',\n",
       " 96: 'pears',\n",
       " 97: 'she',\n",
       " 98: 'little',\n",
       " 99: 'red',\n",
       " 100: 'winter',\n",
       " 101: 'disliked',\n",
       " 102: 'rusty',\n",
       " 103: 'car',\n",
       " 104: 'strawberries',\n",
       " 105: 'i',\n",
       " 106: 'strawberry',\n",
       " 107: 'bananas',\n",
       " 108: 'going',\n",
       " 109: 'to',\n",
       " 110: 'next',\n",
       " 111: 'plan',\n",
       " 112: 'visit',\n",
       " 113: 'elephants',\n",
       " 114: 'were',\n",
       " 115: 'animals',\n",
       " 116: 'are',\n",
       " 117: 'likes',\n",
       " 118: 'dislike',\n",
       " 119: 'fall',\n",
       " 120: 'driving',\n",
       " 121: 'peach',\n",
       " 122: 'drives',\n",
       " 123: 'blue',\n",
       " 124: 'you',\n",
       " 125: 'bird',\n",
       " 126: 'horses',\n",
       " 127: 'mouse',\n",
       " 128: 'went',\n",
       " 129: 'last',\n",
       " 130: 'horse',\n",
       " 131: 'automobile',\n",
       " 132: 'dogs',\n",
       " 133: 'white',\n",
       " 134: 'elephant',\n",
       " 135: 'black',\n",
       " 136: 'think',\n",
       " 137: 'difficult',\n",
       " 138: 'translate',\n",
       " 139: 'between',\n",
       " 140: 'spanish',\n",
       " 141: 'portuguese',\n",
       " 142: 'big',\n",
       " 143: 'green',\n",
       " 144: 'translating',\n",
       " 145: 'fun',\n",
       " 146: 'where',\n",
       " 147: 'dog',\n",
       " 148: 'why',\n",
       " 149: 'might',\n",
       " 150: 'go',\n",
       " 151: 'this',\n",
       " 152: 'drove',\n",
       " 153: 'shiny',\n",
       " 154: 'sharks',\n",
       " 155: 'monkey',\n",
       " 156: 'how',\n",
       " 157: 'weather',\n",
       " 158: 'lion',\n",
       " 159: 'plans',\n",
       " 160: 'bear',\n",
       " 161: 'rabbit',\n",
       " 162: 'its',\n",
       " 163: 'chinese',\n",
       " 164: 'when',\n",
       " 165: 'eiffel',\n",
       " 166: 'tower',\n",
       " 167: 'did',\n",
       " 168: 'grocery',\n",
       " 169: 'store',\n",
       " 170: 'wanted',\n",
       " 171: 'does',\n",
       " 172: 'football',\n",
       " 173: 'field',\n",
       " 174: 'wants',\n",
       " 175: 'didnt',\n",
       " 176: 'snake',\n",
       " 177: 'snakes',\n",
       " 178: 'do',\n",
       " 179: 'easy',\n",
       " 180: 'thinks',\n",
       " 181: 'english',\n",
       " 182: 'french',\n",
       " 183: 'would',\n",
       " 184: 'arent',\n",
       " 185: 'cats',\n",
       " 186: 'rabbits',\n",
       " 187: 'has',\n",
       " 188: 'been',\n",
       " 189: 'monkeys',\n",
       " 190: 'lake',\n",
       " 191: 'bears',\n",
       " 192: 'school',\n",
       " 193: 'birds',\n",
       " 194: 'want',\n",
       " 195: 'isnt',\n",
       " 196: 'lions',\n",
       " 197: 'am',\n",
       " 198: 'mice',\n",
       " 199: 'have'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inverse_vocab = {index: token for token, index in vocab.items()}\n",
    "inverse_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Use a pre-trained Word2Vec model to generate word embeddings for a given text corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We use google's pre trained word2vec model to vectorize the english words given"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in c:\\users\\adity\\anaconda3\\envs\\nullclass\\lib\\site-packages (4.3.2)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\adity\\anaconda3\\envs\\nullclass\\lib\\site-packages (from gensim) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.7.0 in c:\\users\\adity\\anaconda3\\envs\\nullclass\\lib\\site-packages (from gensim) (1.10.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\adity\\anaconda3\\envs\\nullclass\\lib\\site-packages (from gensim) (7.0.4)\n",
      "Requirement already satisfied: wrapt in c:\\users\\adity\\anaconda3\\envs\\nullclass\\lib\\site-packages (from smart-open>=1.8.1->gensim) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = api.load(\"word2vec-google-news-300\", return_path=True) #takes too long to load 3-4min\n",
    "model = KeyedVectors.load_word2vec_format(model_path, binary=True) #otherwise after downloading it takes 48seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_vector(word):\n",
    "    if word in model:\n",
    "        return model[word]\n",
    "    else:\n",
    "        return None  # or return a vector of zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_corpus = []\n",
    "for word in tokens:\n",
    "    vector = get_word_vector(word)\n",
    "    if vector is not None:\n",
    "        vectorized_corpus.append(vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Load a pre-trained LSTM-based NMT model and use it to translate a sentence from one language to another."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We use hugging face's transformer module to load a pre trained LSTM model (Helsinki) to translate from english to french"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/huggingface/transformers\n",
      "  Cloning https://github.com/huggingface/transformers to c:\\users\\adity\\appdata\\local\\temp\\pip-req-build-sv3mzihz\n",
      "  Resolved https://github.com/huggingface/transformers to commit a3fb96a42a9ee473de61ac01a860251123042943\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: filelock in c:\\users\\adity\\anaconda3\\envs\\nullclass\\lib\\site-packages (from transformers==4.42.0.dev0) (3.14.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\adity\\anaconda3\\envs\\nullclass\\lib\\site-packages (from transformers==4.42.0.dev0) (0.23.2)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17 in c:\\users\\adity\\anaconda3\\envs\\nullclass\\lib\\site-packages (from transformers==4.42.0.dev0) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\adity\\anaconda3\\envs\\nullclass\\lib\\site-packages (from transformers==4.42.0.dev0) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\adity\\anaconda3\\envs\\nullclass\\lib\\site-packages (from transformers==4.42.0.dev0) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\adity\\anaconda3\\envs\\nullclass\\lib\\site-packages (from transformers==4.42.0.dev0) (2024.5.15)\n",
      "Requirement already satisfied: requests in c:\\users\\adity\\anaconda3\\envs\\nullclass\\lib\\site-packages (from transformers==4.42.0.dev0) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in c:\\users\\adity\\anaconda3\\envs\\nullclass\\lib\\site-packages (from transformers==4.42.0.dev0) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\adity\\anaconda3\\envs\\nullclass\\lib\\site-packages (from transformers==4.42.0.dev0) (0.4.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\adity\\anaconda3\\envs\\nullclass\\lib\\site-packages (from transformers==4.42.0.dev0) (4.66.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\adity\\anaconda3\\envs\\nullclass\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.42.0.dev0) (2024.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\adity\\anaconda3\\envs\\nullclass\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.42.0.dev0) (4.11.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\adity\\anaconda3\\envs\\nullclass\\lib\\site-packages (from tqdm>=4.27->transformers==4.42.0.dev0) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\adity\\anaconda3\\envs\\nullclass\\lib\\site-packages (from requests->transformers==4.42.0.dev0) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\adity\\anaconda3\\envs\\nullclass\\lib\\site-packages (from requests->transformers==4.42.0.dev0) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\adity\\anaconda3\\envs\\nullclass\\lib\\site-packages (from requests->transformers==4.42.0.dev0) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\adity\\anaconda3\\envs\\nullclass\\lib\\site-packages (from requests->transformers==4.42.0.dev0) (2024.2.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers 'C:\\Users\\adity\\AppData\\Local\\Temp\\pip-req-build-sv3mzihz'\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/huggingface/transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import MarianMTModel, MarianTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_text(text, src_lang, tgt_lang):\n",
    "    model_name = f'Helsinki-NLP/opus-mt-{src_lang}-{tgt_lang}'\n",
    "    tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "    model = MarianMTModel.from_pretrained(model_name)\n",
    "\n",
    "    # Tokenize the input text\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "    # Generate translation\n",
    "    translated_tokens = model.generate(**inputs)\n",
    "\n",
    "    # Decode the translated tokens\n",
    "    translated_text = tokenizer.decode(translated_tokens[0], skip_special_tokens=True)\n",
    "    return translated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_sentence = \"The whistling of the leaves is utterly calming\"\n",
    "source_language = \"en\"  # English\n",
    "target_language = \"fr\"  # French\n",
    "\n",
    "translated_sentence = translate_text(source_sentence, source_language, target_language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le sifflement des feuilles est totalement apaisant\n"
     ]
    }
   ],
   "source": [
    "print(translated_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Implement beam search decoding for an NMT model to improve translation quality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We implement beam search decoding for the NMT model we used in the course curriculum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5: Create a feature to translate the language from French to Tamil\n",
    "\n",
    "#### It should predict if the french word has only five letter if the french word has more than five letters or less than five letters the model should not translate the word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We use facebook/m2m100_418m model, which allows translation directly from french to tamil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in c:\\users\\adity\\anaconda3\\envs\\nullclass\\lib\\site-packages (0.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fr_text = \"La vie est comme une boîte de chocolat.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = M2M100ForConditionalGeneration.from_pretrained(\"facebook/m2m100_418M\")\n",
    "tokenizer = M2M100Tokenizer.from_pretrained(\"facebook/m2m100_418M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_five_letter_words(text):\n",
    "    words = text.split()\n",
    "    filtered_words = [word for word in words if len(word) == 5]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "filtered_fr_text = filter_five_letter_words(fr_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: La vie est comme une boîte de chocolat.\n",
      "Filtered text: comme boîte\n",
      "Translation: ['கோடு போல்']\n"
     ]
    }
   ],
   "source": [
    "if filtered_fr_text:\n",
    "    tokenizer.src_lang = \"fr\"\n",
    "    encoded_fr = tokenizer(filtered_fr_text, return_tensors=\"pt\")\n",
    "    generated_tokens = model.generate(**encoded_fr, forced_bos_token_id=tokenizer.get_lang_id(\"ta\"))\n",
    "    translation = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
    "else:\n",
    "    translation = []\n",
    "\n",
    "print(\"Original text:\", fr_text)\n",
    "print(\"Filtered text:\", filtered_fr_text)\n",
    "print(\"Translation:\", translation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 6: Create a feature to throw an error if we enter the wrong word\n",
    "\n",
    "#### If we enter a word which is not available the program should throw an error saying like this “word is not available” and provide some suggestion related to the word which is incorrect . If the user enter continuously 2 wrong word it should show list of wrong words which we enter so far in the error notification as well as it should give some suggestions related with wrong word ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
